# -*- coding: utf-8 -*-
"""Advanced DL Project Notebook - Group 10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WC98cV2I-l3FTWGnK_tqRxKF54vlNS0U

# Upload Data from Kaggle
"""

import os
os.environ['KAGGLE_CONFIG_DIR'] = '/content'

import locale
locale.getpreferredencoding = lambda: "UTF-8"

!kaggle datasets download -d therohk/ireland-historical-news

from zipfile import ZipFile

with ZipFile('/content/ireland-historical-news.zip', 'r') as zipObj:
  zipObj.extractall('/content')

"""# Read data"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
from collections import Counter
import plotly.express as px
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

!pip install textacy

## for data
import json
import pandas as pd
import numpy as np
from sklearn import metrics, manifold
## for processing
import re
import nltk
## for plotting
import matplotlib.pyplot as plt
import seaborn as sns
## for nlp
import spacy  #3.5.0
from spacy import displacy
import textacy  #0.12.0

## for graph
import networkx as nx

headlines_df = pd.read_csv('/content/ireland-news-headlines.csv')

headlines_df

headlines_df = headlines_df.dropna(subset=['headline_text'])

"""# Data exploration

Turn publish date to pd.datetime
"""

headlines_df['publish_date'] = pd.to_datetime(headlines_df['publish_date'].apply(lambda date: str(date)[:4] + '-' + str(date)[4:6] + '-' + str(date)[6:]))

"""Subtrcact main category and sub-categories"""

headlines_df['main_category'] = headlines_df['headline_category'].apply(lambda cat: cat.split('.')[0] if '.' in cat else cat)

headlines_df

"""# Categories # Analysis

"""

category_counts = headlines_df.groupby('main_category').count()['headline_text']
category_percents = category_counts.apply(lambda x: x / category_counts.sum() * 100)
fig = px.pie(names=category_percents.index, values=category_percents.values, title='Percentage of Headlines by Category')
fig.show()

"""# Trend over time"""

headlines_df['year'] = headlines_df['publish_date'].apply(lambda date: date.year)

# Group the headlines by year and main category
df = headlines_df.groupby(['year', 'main_category']).count().reset_index()

# Create the stacked bar chart
fig = px.bar(df, x='year', y='headline_text', color='main_category')

# Set the chart title and axis labels
fig.update_layout(title='Number of Headlines by Main Category Over Time per Year',
                  xaxis_title='Year',
                  yaxis_title='Number of Headlines')

# Show the chart
fig.show()

"""# Top words per category"""

'''
Preprocess a string.
:parameter
    :param text: string - name of column containing text
    :param lst_stopwords: list - list of stopwords to remove
    :param flg_stemm: bool - whether stemming is to be applied
    :param flg_lemm: bool - whether lemmitisation is to be applied
:return
    cleaned text
'''
def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):
    ## clean (convert to lowercase and remove punctuations and characters and then strip)
    text = re.sub(r'[^\w\s]', '', str(text).lower().strip())

    ## Tokenize (convert from string to list)
    lst_text = text.split()
    ## remove Stopwords
    if lst_stopwords is not None:
        lst_text = [word for word in lst_text if word not in
                    lst_stopwords]

    ## Stemming (remove -ing, -ly, ...)
    if flg_stemm == True:
        ps = nltk.stem.porter.PorterStemmer()
        lst_text = [ps.stem(word) for word in lst_text]

    ## Lemmatisation (convert the word into root word)
    if flg_lemm == True:
        lem = nltk.stem.wordnet.WordNetLemmatizer()
        lst_text = [lem.lemmatize(word) for word in lst_text]

    return lst_text

lst_stopwords = nltk.corpus.stopwords.words("english")

headlines_df["clean_text"] = headlines_df["headline_text"].apply(lambda x:
          utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,
          lst_stopwords=lst_stopwords))
headlines_df

# Group the data by main category
groups = headlines_df.groupby('main_category')

# Define the number of top words to select
num_top_words = 100

# Create a new dataframe that contains the top words and their frequencies for each main category
top_words_df = pd.DataFrame(columns=['category', 'word', 'freq'])
for name, group in groups:
    # Create a bag of words by combining all the headline text for this category
    bag_of_words = [word for headline in group['clean_text'] for word in headline]
    # Count the frequency of each word
    word_freq = Counter(bag_of_words)
    # Sort the words by frequency in descending order
    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
    # Select the top words and their frequencies
    top_words = sorted_words[:num_top_words]
    # Add the top words and their frequencies to the new dataframe
    for word, freq in top_words:
        top_words_df = top_words_df.append({'category': name, 'word': word, 'freq': freq}, ignore_index=True)

# Create the treemap with Plotly
fig = px.treemap(top_words_df, path=['category', 'word'], values='freq', color='freq', color_continuous_scale='Greens')
fig.update_layout(title='Top Words by Category')
fig.show()

# Exclude ["irish", "dublin", "ireland"] words and "new"
top_words_df = top_words_df.query('word not in ["irish", "dublin", "ireland", "new"]')

# Create the treemap with Plotly
fig = px.treemap(top_words_df, path=['category', 'word'], values='freq', color='freq', color_continuous_scale='Greens')
fig.update_layout(title='Top Words by Category')
fig.show()

"""# Sentiment"""

from textblob import TextBlob


# Define a function to calculate the sentiment polarity of a given headline
def get_sentiment(text):
  if not pd.isnull(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity

# Apply the get_sentiment function to each headline text in the dataframe
headlines_df['sentiment'] = headlines_df['headline_text'].apply(get_sentiment)

# Create a histogram of the sentiment polarity values
fig = px.histogram(headlines_df, x='sentiment', nbins=20, color_discrete_sequence=['blue'])
fig.update_layout(title='Distribution of Sentiment Polarity in Headlines',
                  xaxis_title='Sentiment Polarity', yaxis_title='Count')
fig.show()

sentiment_by_category = headlines_df.groupby('main_category')['sentiment'].mean().reset_index().sort_values('sentiment', ascending=False)

# Create the bar plot with a green-to-red color scale
fig = px.bar(sentiment_by_category, x='main_category', y='sentiment',
             title='Average Sentiment Score by Main Category',
             color='sentiment',
             color_continuous_scale='RdYlGn')
fig.show()

"""# Entities Anlysis using Networx"""

nlp = spacy.load("en_core_web_sm")
sample_df = headlines_df.sample(n=10000)
indexes = sample_df.index
clean_text_as_lst = sample_df['clean_text'].tolist()

## extract entities and relations
dic = {"id":[], "category":[], "text":[], "entity":[], "relation":[], "object":[]}
i = -1
for n,sentence in enumerate(clean_text_as_lst):
    i += 1
    sentence_index = indexes[i]
    sentence = " ".join(sentence)
    sentence = nlp(sentence)
    lst_generators = list(textacy.extract.subject_verb_object_triples(sentence))
    for sent in lst_generators:
        subj = "_".join(map(str, sent.subject))
        obj  = "_".join(map(str, sent.object))
        relation = "_".join(map(str, sent.verb))
        dic["id"].append(n)
        category = headlines_df['main_category'].iloc[sentence_index]
        dic["category"].append(category)
        dic["text"].append(sentence.text)
        dic["entity"].append(subj)
        dic["object"].append(obj)
        dic["relation"].append(relation)


## create dataframe
dtf = pd.DataFrame(dic)

dtf.head()

"""# Graph"""

categories = dtf['category'].unique()
for cat in categories:
  curr_cat_df = dtf[dtf['category'] == cat]
  most_mentioned_entity = curr_cat_df["entity"].value_counts().index[0]
  tmp = curr_cat_df[(curr_cat_df["entity"]==most_mentioned_entity) | (curr_cat_df["object"]==most_mentioned_entity)]
  ## create small graph
  G = nx.from_pandas_edgelist(tmp, source="entity", target="object",
                              edge_attr="relation",
                              create_using=nx.DiGraph())


  ## plot
  plt.figure(figsize=(15,10))

  pos = nx.spring_layout(G, k=1)
  node_color = ["red" if node==most_mentioned_entity else "skyblue" for node in G.nodes]
  edge_color = ["red" if edge[0]==most_mentioned_entity else "black" for edge in G.edges]

  nx.draw(G, pos=pos, with_labels=True, node_color=node_color,
          edge_color=edge_color, cmap=plt.cm.Dark2,
          node_size=2000, node_shape="o", connectionstyle='arc3,rad=0.1')

  nx.draw_networkx_edge_labels(G, pos=pos, label_pos=0.5,
                          edge_labels=nx.get_edge_attributes(G,'relation'),
                          font_size=12, font_color='black', alpha=0.6)
  plt.title(f"{cat.capitalize()} - Most Common Entity Relations")
  plt.show()

"""# Clean Data for Model"""

headlines_df['clean_text_for_y'] = headlines_df['clean_text'].apply(lambda text: " ".join(text))

headlines_df

"""## Take only 5% of the data beacuase it is too much data for COLAB GPU for training, and divide it evenly between the categories"""

sample_ratio = 0.05
category_counts = headlines_df['main_category'].value_counts()
smallest_category_count = category_counts.min()
sample_size = int(sample_ratio * smallest_category_count)
sampled_data = pd.DataFrame()
for category in category_counts.index:
    category_data = headlines_df[headlines_df['main_category'] == category].sample(sample_size, random_state=42)
    sampled_data = pd.concat([sampled_data, category_data])
sampled_data = sampled_data.reset_index(drop=True)

category_counts = sampled_data.groupby('main_category').count()['headline_text']
category_percents = category_counts.apply(lambda x: x / category_counts.sum() * 100)
fig = px.pie(names=category_percents.index, values=category_percents.values, title='Percentage of Headlines by Category')
fig.show()

category_counts = sampled_data['main_category'].value_counts()
fig = px.bar(x=category_counts.index, y=category_counts.values, labels={'x':'Main Category', 'y':'Number of Records'})
fig.show()

"""# Part II - Finetune Models"""

!pip install transformers

!pip install sentencepiece

import torch
import transformers
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np

from tabulate import tabulate
from tqdm import trange
import random

"""# Example of Bert tokenizer implamantation"""

tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

category_mapping = {category: i for i, category in enumerate(sampled_data['main_category'].unique())}
sampled_data['category_numeric'] = sampled_data['main_category'].map(category_mapping)

sampled_data

text = sampled_data['clean_text_for_y'].values
labels = sampled_data['category_numeric'].values

def print_rand_sentence():
  '''Displays the tokens and respective IDs of a random text sample'''
  index = random.randint(0, len(text)-1)
  table = np.array([tokenizer.tokenize(text[index]),
                    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[index]))]).T
  print(table)

print_rand_sentence()

token_id = []
attention_masks = []

def preprocessing(input_text, tokenizer):
  '''
  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:
    - input_ids: list of token ids
    - token_type_ids: list of token type ids
    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).
  '''
  return tokenizer.encode_plus(
                        input_text,
                        add_special_tokens = True,
                        max_length = 16,
                        pad_to_max_length = True,
                        return_attention_mask = True,
                        return_tensors = 'pt'
                   )


for sample in text:
  encoding_dict = preprocessing(sample, tokenizer)
  token_id.append(encoding_dict['input_ids'])
  attention_masks.append(encoding_dict['attention_mask'])


token_id = torch.cat(token_id, dim = 0)
attention_masks = torch.cat(attention_masks, dim = 0)
labels = torch.tensor(labels)

def print_rand_sentence_encoding():
  '''Displays tokens, token IDs and attention mask of a random text sample'''
  index = random.randint(0, len(text) - 1)
  tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))
  token_ids = [i.numpy() for i in token_id[index]]
  attention = [i.numpy() for i in attention_masks[index]]

  table = np.array([tokens, token_ids, attention]).T
  print(tabulate(table,
                 headers = ['Tokens', 'Token IDs', 'Attention Mask'],
                 tablefmt = 'fancy_grid'))

print_rand_sentence_encoding()

"""# Models Training Robust Mechanism

"""

sampled_data

import torch
import torch.nn.utils.prune as prune
import torch.quantization
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import DataLoader, TensorDataset

# Set the random seed for reproducibility
torch.manual_seed(42)

# Load the dataset and split into training and testing setsy
X_train, X_test, y_train, y_test = train_test_split(sampled_data['clean_text_for_y'], sampled_data['main_category'], test_size=0.2, random_state=42)

from tqdm import tqdm
class Model():
  """
    A class representing a DL NLP pre-trained model.

    Attributes:
        model_name (str): The name of the model.

   Methods:
        __init__(model_name): Initializes a new instance of the Model class.
        fit(epcohs): Trains the model on the given training data with selected number of epochs.
        evaluate(): Evaluates the model on the validation data and plots a confusion matrix.
        predict(headline): Predicts the main category label for a given headline.

    Example:
        model = Model('albert-base-v2')
    """

  def __init__(self, model_name) -> None:
     self.model_name = model_name
     self.teacher_model_name = 'distilbert-base-uncased'
     self.student_model_name = model_name
     self.teacher_model = None
     self.student_model = None

  def prune(self, pruning_ratio, save_path):
        """
        Applies magnitude-based pruning to the model.

        Args:
            pruning_ratio (float): The ratio of weights to prune.

        Returns:
            None
        """
        # Load Trained Model
        self.load_model(save_path)
        # Define the pruning method
        pruning_method = prune.L1Unstructured

        # Create the pruning mask for each applicable module in the model
        for module in self.model.modules():
            if isinstance(module, torch.nn.Linear):
                prune.random_unstructured(module, name="weight", amount=pruning_ratio)

  def quantize_model(self, save_path):
        """
        Applies quantization to the model.

        Returns:
            None
        """
        # Load Trained Model
        self.load_model(save_path)
        # Perform quantize
        quantized_model = torch.quantization.quantize_dynamic(
            self.model, {torch.nn.Linear}, dtype=torch.qint8
        )
        self.model = quantized_model

  def distill(self, temperature, alpha):
        """
        Performs distillation with a teacher-student method.

        Args:
            temperature (float): The temperature for distillation.
            alpha (float): The weight of the distillation loss.

        Returns:
            None
        """
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # Initialize the tokenizer and encode the headlines
        tokenizer = AutoTokenizer.from_pretrained(self.student_model_name)
        encoded_inputs_train = tokenizer(list(X_train), truncation=True, padding=True)

        # Convert the labels to numerical format
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train)

        # Create PyTorch tensors from the encoded inputs and labels
        train_inputs = torch.tensor(encoded_inputs_train['input_ids'])
        train_masks = torch.tensor(encoded_inputs_train['attention_mask'])
        train_labels = torch.tensor(y_train_encoded)

        # Create a DataLoader for the training set
        train_dataset = TensorDataset(train_inputs, train_masks, train_labels)
        train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)
        # Load the pre-trained teacher model
        self.teacher_model = AutoModelForSequenceClassification.from_pretrained(self.teacher_model_name)
        self.teacher_model.to(device)
        # Load the student model architecture
        self.student_model = AutoModelForSequenceClassification.from_pretrained(self.student_model_name)
        self.student_model.to(device)

        # Set the models to evaluation mode
        self.teacher_model.eval()
        self.student_model.train()

        # Define the optimizer and learning rate
        optimizer = torch.optim.AdamW(self.student_model.parameters(), lr=2e-5)

        # Set the device to GPU if available, otherwise use CPU
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


        # Define the loss function
        loss_fn = torch.nn.KLDivLoss(reduction='batchmean')

        # Training loop
        for epoch in range(3):
            for batch in train_dataloader:
                inputs, masks, labels = batch
                inputs = inputs.to(device)
                masks = masks.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()

                # Forward pass with the teacher model
                with torch.no_grad():
                    teacher_outputs = self.teacher_model(inputs, masks).logits

                # Forward pass with the student model
                student_outputs = self.student_model(inputs, masks).logits

                # Apply temperature scaling to the teacher outputs
                teacher_outputs /= temperature
                student_outputs /= temperature

                # Calculate the distillation loss
                distillation_loss = loss_fn(F.log_softmax(student_outputs, dim=1), F.softmax(teacher_outputs, dim=1))

                # Calculate the classification loss
                classification_loss = F.cross_entropy(student_outputs, labels)

                # Calculate the total loss
                total_loss = alpha * distillation_loss + (1 - alpha) * classification_loss

                # Backward pass and optimization step
                total_loss.backward()
                optimizer.step()

  def fit(self, original_model=False, epochs=3):
    """
    Trains the model on the given training data.

    Args:
        epochs = Number of selected epochs

    Returns:
        None
    """
    # Initialize the tokenizer and encode the headlines
    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
    encoded_inputs_train = self.tokenizer(list(X_train), truncation=True, padding=True)
    encoded_inputs_test = self.tokenizer(list(X_test), truncation=True, padding=True)

    # Convert the labels to numerical format
    self.label_encoder = LabelEncoder()
    y_train_encoded = self.label_encoder.fit_transform(y_train)
    y_test_encoded = self.label_encoder.transform(y_test)

    # Create PyTorch tensors from the encoded inputs and labels
    train_inputs = torch.tensor(encoded_inputs_train['input_ids'])
    train_masks = torch.tensor(encoded_inputs_train['attention_mask'])
    train_labels = torch.tensor(y_train_encoded)
    test_inputs = torch.tensor(encoded_inputs_test['input_ids'])
    test_masks = torch.tensor(encoded_inputs_test['attention_mask'])
    test_labels = torch.tensor(y_test_encoded)

    # Create a DataLoader for training and testing sets
    train_dataset = TensorDataset(train_inputs, train_masks, train_labels)
    test_dataset = TensorDataset(test_inputs, test_masks, test_labels)
    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    test_dataloader = DataLoader(test_dataset, batch_size=16)

    if original_model:
      # Initialize the model for sequence classification
      self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=len(self.label_encoder.classes_))

    # Set the device to GPU if available, otherwise use CPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    self.model.to(device)

    # Define the optimizer and learning rate
    optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)
    self.preds_labels = []
    self.true_labels = []
    # Training loop
    self.model.train()
    for epoch in tqdm(range(epochs), desc="Epochs"):
        for batch in train_dataloader:
            inputs, masks, labels = batch
            inputs = inputs.to(device)
            masks = masks.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()

            outputs = self.model(input_ids=inputs, attention_mask=masks, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()

    # Evaluation
    self.model.eval()
    total_correct = 0
    total_samples = 0
    with torch.no_grad():
        for batch in test_dataloader:
            inputs, masks, labels = batch
            inputs = inputs.to(device)
            masks = masks.to(device)
            labels = labels.to(device)

            outputs = self.model(input_ids=inputs, attention_mask=masks)
            _, predicted_labels = torch.max(outputs.logits, dim=1)

            total_correct += (predicted_labels == labels).sum().item()
            total_samples += labels.size(0)
            self.preds_labels.append(list(predicted_labels.cpu()))
            self.true_labels.append(list(labels.cpu()))
    if original_model:
      self.save_model()

  def evaluate(self, name):
    """
      Evaluates the model on the validation data and plots a confusion matrix.
      Returns:Model Accuracy
    """
    cat_num_dict = {}
    for num_cat in sampled_data['category_numeric'].unique():
      cat_num_dict[num_cat] = sampled_data['main_category'].unique()[num_cat]
    preds, label_ids = np.array(self.preds_labels).flatten(), np.array(self.true_labels).flatten()
    predicted, y_test = [cat_num_dict[pred] for pred in preds], [cat_num_dict[label] for label in label_ids]
    classes = np.unique(y_test)
    y_test_array = pd.get_dummies(y_test, drop_first=False).values

    ## Accuracy, Precision, Recall
    accuracy = metrics.accuracy_score(y_test, predicted)
    print("\n\n\nValidation Accuracy:",  round(accuracy,2))
    print("Detail:")
    print(metrics.classification_report(y_test, predicted))

    ## Plot confusion matrix
    cm = metrics.confusion_matrix(y_test, predicted)
    fig, ax = plt.subplots()
    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues,
                cbar=False)
    model_new_name = self.model_name.capitalize() + f' {name.capitalize()}'
    ax.set(xlabel="Pred", ylabel="True", xticklabels=classes,
          yticklabels=classes, title=f"{model_new_name} Confusion matrix")

    plt.show()
    return round(accuracy, 2)

  def predict(self, headline):
    """
    Predicts the main category label for a given headline.

    Args:
        headline (str): The headline text to predict the main category label for.

    Returns:
        str: The predicted main category label.
    """
    # Tokenize the headline
    inputs = self.tokenizer(headline, truncation=True, padding=True, return_tensors='pt')

    # Set the device to GPU if available, otherwise use CPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    inputs = {key: value.to(device) for key, value in inputs.items()}

    # Make the prediction
    with torch.no_grad():
        self.model.eval()

        outputs = self.model(**inputs)
        predicted_labels = torch.argmax(outputs.logits, dim=1).cpu().numpy()

    # Convert the predicted label to the corresponding main category
    predicted_category = self.label_encoder.inverse_transform(predicted_labels)[0]
    print(f'Headline: "{headline}"')
    print(f"Predicted Category: {str(predicted_category).capitalize()}\n")

  def predict_headlines(self):
    # Try some headlines from my own :)
    self.predict('Nuggets won the NBA')
    self.predict('What a Motherless Son Knows About Fatherhood')
    self.predict('Ukraines offensive is underway. Heres whats happened so far')
    self.predict('Fed takes a timeout')
    self.predict('Chronic stress can affect your health. One activity can help')
    self.predict('The Arabic language is vital to educational systems of the future')

  def save_model(self, save_dir='/content'):
        if self.model is None:
            raise ValueError("No model has been trained. Please fit the model before saving.")

        os.makedirs(save_dir, exist_ok=True)
        save_path = os.path.join(save_dir, "trained_model.pt")
        torch.save(self.model.state_dict(), save_path)

  def load_model(self, save_path):
        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=self.model.config.num_labels)
        self.model.load_state_dict(torch.load(save_path))

# Finetune Models
model_names = ['bert-base-uncased', 'roberta-base', 'albert-base-v2', 'google/electra-base-generator', 'microsoft/deberta-v3-base']
model_names = ['bert-base-uncased', 'albert-base-v2', 'microsoft/deberta-v3-base']
save_path = '/content/trained_model.pt'
model_types = ['Original', 'Pruned', 'Quantized', 'Distilled']

accuracy_results = []
model_index_names = []

for model_name in model_names:
  model_results = []
  # # Original train & Save model
  curr_model = Model(model_name)
  curr_model.fit(epochs=2, original_model=True)
  original_accuracy = curr_model.evaluate(name='original')
  curr_model.predict_headlines()
  model_results.append(original_accuracy)
  model_index_names.append(curr_model.model_name.capitalize())

  # Apply pruning
  pruning_ratio = 0.5  # Example pruning ratio of 50%
  curr_model.prune(pruning_ratio, save_path)
  # Retrain the pruned model
  curr_model.fit(epochs=2)
  pruned_accuracy = curr_model.evaluate(name='pruned')
  curr_model.predict_headlines()
  model_results.append(pruned_accuracy)
  model_name_pruned = curr_model.model_name.capitalize() + ' Pruned'
  model_index_names.append(model_name_pruned)

  # Apply qunatization
  curr_model.quantize_model(save_path)
  # Retrain the quantized model
  curr_model.fit(epochs=2)
  quantized_accuracy = curr_model.evaluate(name='quantized')
  curr_model.predict_headlines()
  model_results.append(quantized_accuracy)
  model_name_quantized = curr_model.model_name.capitalize() + ' Quantized'
  model_index_names.append(model_name_quantized)

  # Apply distill
  curr_model.distill(temperature=2.0, alpha=0.5)
  # Retrain the quantized model
  curr_model.fit(epochs=2)
  quantized_accuracy = curr_model.evaluate(name='distilled')
  curr_model.predict_headlines()
  model_results.append(quantized_accuracy)
  model_name_distilled = curr_model.model_name.capitalize() + ' Distilled'
  model_index_names.append(model_name_distilled)

  accuracy_results.append(model_results)

accuracy_results, model_types, model_index_names

model_index_names = ['Bert', 'Alberta', 'Deberta']

results_df = pd.DataFrame(data=accuracy_results, columns=model_types, index=model_index_names)

print(results_df)

results_df.to_excel('Models Accuracy Results.xlsx')

